# üè¶  ETL de A√ß√µes e Fundos Imobili√°rios

<div align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-blue?style=for-the-badge&logo=python&logoColor=white" alt="Python">
  
  <img src="https://img.shields.io/badge/Pandas-2.3-150458?style=for-the-badge&logo=pandas&logoColor=white" alt="Pandas">
  
  <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker">
</div>

## üìã Vis√£o Geral

Pipeline para extra√ß√£o, an√°lise e rankeamento de ativos financeiros (FIIs e A√ß√µes) com base em indicadores fundamentais.

## üõ†Ô∏è Tecnologias

- **Banco de Dados:** PostgreSQL
- **Orquestra√ß√£o:** Apache Airflow
- **ETL:** SQL, Pandas
- **An√°lise de Dados:** Pandas
- **Web Scraping:** Scrapy
- **Containeriza√ß√£o:** Docker, Docker Compose

## üèóÔ∏è Arquitetura ETL

O fluxo de ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento) √© orquestrado pelo Apache Airflow, garantindo a confiabilidade e rastreabilidade de todo o processo. A arquitetura segue o seguinte fluxo:

1. **Extra√ß√£o**
   - Utiliza√ß√£o do Scrapy para coleta de dados de fontes financeiras
   - Dados brutos s√£o armazenados temporariamente em formato estruturado

2. **Transforma√ß√£o**
   - Limpeza e normaliza√ß√£o dos dados brutos
   - C√°lculo de indicadores financeiros
   - Valida√ß√£o e tratamento de dados ausentes
   - Aplica√ß√£o de regras de neg√≥cio

3. **Carregamento**
   - Cria√ß√£o autom√°tica de schemas e tabelas no PostgreSQL
   - Carga incremental dos dados processados
   - Manuten√ß√£o de hist√≥rico para an√°lise temporal

![Arquitetura ETL](pics/arquitetura.svg)

O Airflow gerencia todo o fluxo com DAGs (Directed Acyclic Graphs) que s√£o agendadas e monitoradas, garantindo que cada etapa seja executada na ordem correta e com tratamento de falhas adequado.


## Requisitos

- Astronomer-cli - [Download](https://www.astronomer.io/docs/astro/cli/install-cli) 
- Docker - [Download](https://www.docker.com/get-started/) 

## Como usar

Clone o reposit√≥rio em sua m√°quina local

```bash
git clone https://github.com/fabiolucasz/pipeline-acoes-airflow.git
```


## Executar pipelines com astronomer

- Entre na pasta airflow

```bash
cd airflow
```

- Execute o seguinte comando


```bash
astro dev start
```

- Crie as conex√µes com o seu banco de dados de desenvolvimento e produ√ß√£o

- Altere o nome das vari√°veis `conn_id` no arquivo `airflow/dags/dbt_dag.py` para os nomes das conex√µes que voc√™ criou

- Crie uma vari√°vel com o nome `dbt_env` e valor `dev` ou `prod` para alterar dinamicamente entre ambientes de desenvolvimento e produ√ß√£o.


## üìÑ Licen√ßa

Distribu√≠do sob a licen√ßa MIT. Veja `LICENSE` para mais informa√ß√µes.

## ‚úâÔ∏è Contato

Fabio Lucas - [LinkedIn](https://www.linkedin.com/in/fabiolucamz/)

Link do Projeto: [https://github.com/fabiolucasz/pipelina-acoes-airflow](https://github.com/fabiolucasz/pipeline-acoes-airflow)

## üìå Agradecimentos

- [Django](https://www.djangoproject.com/)
- [Bootstrap](https://getbootstrap.com/)
- [Pandas](https://pandas.pydata.org/)
- [Scikit-learn](https://scikit-learn.org/)
- [Todos os contribuidores](../../contributors)
